{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Learning to Rank\n",
    "Bryan K Woods, Ph.D.\n",
    "\n",
    "September 20, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Learning to Rank\" is a type of machine learning problem that focuses on information retrieval. Think of designing a search engine or recommendation system. You use such systems every day when you conduct a Google search for information, shop on Amazon, or look for a destination in your GPS navigation.\n",
    "\n",
    "A [brief overview](https://en.wikipedia.org/wiki/Learning_to_rank) is available on Wikipedia. A search system design generally breaks down into the following steps:\n",
    "1. The user interface (UI) or API receives a search request and submit it to the controller.\n",
    "2. The controller parses the query, \"[tokenizes](https://queryunderstanding.com/tokenization-c8cdd6aef7ff)\" the search terms\n",
    "3. One or more document indices are searched using pre-calculated indexes to match the search tokens. A list of possible matches is returned to the controller.\n",
    "4. The controller sorts the results using a ranking algorithm. Optionally these results are thresholded to eliminate irrelevant or embarassing results.\n",
    "5. The controller returns the sorted results and logs the user response.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/f/fa/MLR-search-engine-example.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "At the simplest level, you can think of this as breaking up the search query from a string to individual words. The process is more complicated than that as each token may not be a word, may be mispelled, and may have multiple meanings. Some examples include:\n",
    "- Abbreviations and acronyms\n",
    "- Synonyms and homonyms\n",
    "- Translations from other languages\n",
    "- Quotations, pluses, and minuses\n",
    "- Numbers (spelled vs digits)\n",
    "- Punctuation and diacriticals\n",
    "- Spell correction\n",
    "- Keywords (near, like, etc) can trigger rules-based special behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate generation\n",
    "\n",
    "- A document index (e.g. lucene) is searched to match the search tokens\n",
    "- A machine-learned weighting function calculates the top-k results\n",
    "- Retrieval speed is very important so feature calculates and model complexity are kept to a minimum\n",
    "- Often just a linear dot product of the feature vector and model vector to produce a relevance score\n",
    "\n",
    "Often there is more than one index due to different types of documents, or because sharding of the index is necessary. In the case of heterogeneous index types, the top-k results from each are returned. In the case of sharding, results are combined before thresholding for the top-k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking model\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "With a limited number of candidates in hand, the are ranked using more complicate feature calculations and models. Features generally fall into four categories:\n",
    "- Static document features that are pre-computed and stored (e.g. prominence of document, size of document, creation date)\n",
    "- Query features (e.g. number of tokens, length of search string, date-time of query)\n",
    "- User-specific features (e.g. user preferences, demographics)\n",
    "- Dynamic features that combine the categories above (e.g. token matching, geographic distance)\n",
    "Feature engineering is the most important part of ranking process.\n",
    "\n",
    "### Ranking strategies\n",
    "\n",
    "#### Pointwise\n",
    "For each document, calculate a relevance score. Sort and optionally threshold based on this score. The scoring function can either be a regressor to calculate the score directly, or a binary classifier that calculates a probability that the document is relevant.\n",
    "\n",
    "#### Pairwise\n",
    "- Compare each document to the other documents in the list\n",
    "- We train and evaluate against the difference of the feature vectors\n",
    "- Train a binary classifier to predict if one document is preferrable to another\n",
    "- The goal is to minimize the number of inversions (a more prefered document below less preferred one) in the ranking\n",
    "\n",
    "Note that pairwise preference functions are non-transitive. If A > B, and B > C, that does *not* mean that A > C. But if we falsely assume transitivity, we can directly use the preference function as a comparator for sorting O(n log n). Otherwise we must use \"tournament ranking\" O(n<sup>2</sup>) to compare every document to every other document and sorted based on the number of times each document was preferred.\n",
    "\n",
    "#### Listwise\n",
    "A limitation of pairwise ranking is that it does not distinguish between pair inversions in the top or bottom of the results list. For most applications it is more important to get the top of the list correct as usually the user will never see the bottom of the results list. An alternative approach is to use either a pointwise or pairwise method, but instead optimize a metric based on the entire list, possibly biased toward the top of the list like [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) or [ERR](http://olivier.chapelle.cc/pub/err.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "### Load the data\n",
    "Ranking is supported by XGBoost, though the [documentation and examples](https://github.com/dmlc/xgboost/tree/master/demo/rank) are poor.\n",
    "\n",
    "For this demo we use the [LETOR 4.0 / MQ2008](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/)\n",
    "dataset. This the demo ranking dataset that comes with XGBoost and is commonly used in Learning to Rank tutorials.\n",
    "This dataset is downloaded as part of the included container build.\n",
    "\n",
    "The MQ2008 data provided in this demo has been transformed to [SVMLight](http://svmlight.joachims.org/) format. This is\n",
    "a standard machine learning data format that support labels, sparse feature vectors, and an enumerated group (e.g.,\n",
    "query ID). It can be easilly [read using scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the input data\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "import numpy as np\n",
    "\n",
    "svmlight_files = ['data/mq2008.{}'.format(s) for s in ['train', 'test', 'vali']]\n",
    "# labels and feature vectors are in the SVMLight files\n",
    "X_train, y_train, X_test, y_test, X_vali, y_vali = load_svmlight_files(svmlight_files)\n",
    "# while SVMLight supports a qid (group) label, this data set instead has seperate group files\n",
    "grp_train, grp_test, grp_vali = [np.loadtxt(\"{}.group\".format(s)) for s in svmlight_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than using its sklearn API, we'll use it's python API directly\n",
    "# it's data format is called the DMatrix\n",
    "import xgboost as xgb\n",
    "\n",
    "Dtrain = xgb.DMatrix(X_train, y_train)\n",
    "Dtrain.set_group(grp_train.astype(int))\n",
    "\n",
    "Dtest = xgb.DMatrix(X_test, y_test)\n",
    "Dtest.set_group(grp_test.astype(int))\n",
    "\n",
    "Dvali = xgb.DMatrix(X_vali, y_vali)\n",
    "Dvali.set_group(grp_vali.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-ndcg@5:0.71344\ttest-ndcg:0.79072\n",
      "Multiple eval metrics have been passed: 'test-ndcg' will be used for early stopping.\n",
      "\n",
      "Will train until test-ndcg hasn't improved in 10 rounds.\n",
      "[1]\ttest-ndcg@5:0.75125\ttest-ndcg:0.82166\n",
      "[2]\ttest-ndcg@5:0.75652\ttest-ndcg:0.82362\n",
      "[3]\ttest-ndcg@5:0.75769\ttest-ndcg:0.82468\n",
      "[4]\ttest-ndcg@5:0.76401\ttest-ndcg:0.83122\n",
      "[5]\ttest-ndcg@5:0.76390\ttest-ndcg:0.83047\n",
      "[6]\ttest-ndcg@5:0.76332\ttest-ndcg:0.83120\n",
      "[7]\ttest-ndcg@5:0.75790\ttest-ndcg:0.82581\n",
      "[8]\ttest-ndcg@5:0.75808\ttest-ndcg:0.82771\n",
      "[9]\ttest-ndcg@5:0.76141\ttest-ndcg:0.83042\n",
      "[10]\ttest-ndcg@5:0.75530\ttest-ndcg:0.82542\n",
      "[11]\ttest-ndcg@5:0.75563\ttest-ndcg:0.82542\n",
      "[12]\ttest-ndcg@5:0.75885\ttest-ndcg:0.82525\n",
      "[13]\ttest-ndcg@5:0.76363\ttest-ndcg:0.82600\n",
      "[14]\ttest-ndcg@5:0.76429\ttest-ndcg:0.82981\n",
      "Stopping. Best iteration:\n",
      "[4]\ttest-ndcg@5:0.76401\ttest-ndcg:0.83122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the ranking parameters\n",
    "params = {'objective': 'rank:pairwise',\n",
    "          'eval_metric': ['ndcg@5','ndcg']\n",
    "          }\n",
    "\n",
    "# for training the model, we will give it a test set for early-stopping\n",
    "# the num_boost_rounds defines the absolute max, but it will stop when it shows no further improvement\n",
    "xgb_pairwise = xgb.train(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, evals=[(Dtest, 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listwise vs Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-ndcg@5:0.73454\ttest-ndcg:0.80988\n",
      "Multiple eval metrics have been passed: 'test-ndcg' will be used for early stopping.\n",
      "\n",
      "Will train until test-ndcg hasn't improved in 10 rounds.\n",
      "[1]\ttest-ndcg@5:0.75415\ttest-ndcg:0.82786\n",
      "[2]\ttest-ndcg@5:0.75398\ttest-ndcg:0.82786\n",
      "[3]\ttest-ndcg@5:0.76699\ttest-ndcg:0.83606\n",
      "[4]\ttest-ndcg@5:0.76457\ttest-ndcg:0.83210\n",
      "[5]\ttest-ndcg@5:0.75897\ttest-ndcg:0.82627\n",
      "[6]\ttest-ndcg@5:0.76113\ttest-ndcg:0.82821\n",
      "[7]\ttest-ndcg@5:0.76616\ttest-ndcg:0.82799\n",
      "[8]\ttest-ndcg@5:0.77049\ttest-ndcg:0.83358\n",
      "[9]\ttest-ndcg@5:0.77007\ttest-ndcg:0.83279\n",
      "[10]\ttest-ndcg@5:0.76863\ttest-ndcg:0.83050\n",
      "[11]\ttest-ndcg@5:0.76835\ttest-ndcg:0.83005\n",
      "[12]\ttest-ndcg@5:0.76465\ttest-ndcg:0.82766\n",
      "[13]\ttest-ndcg@5:0.76706\ttest-ndcg:0.83094\n",
      "Stopping. Best iteration:\n",
      "[3]\ttest-ndcg@5:0.76699\ttest-ndcg:0.83606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can try optimizing NDCG directly instead\n",
    "\n",
    "# define the ranking parameters\n",
    "params = {'objective': 'rank:ndcg',\n",
    "          'eval_metric': ['ndcg@5','ndcg']\n",
    "          }\n",
    "\n",
    "# for training the model, we will give it a test set for early-stopping\n",
    "# the num_boost_rounds defines the absolute max, but it will stop when it shows no further improvement\n",
    "xgb_listwise = xgb.train(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, evals=[(Dtest, 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_importance(xgb_listwise)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 features reduced to 33\n"
     ]
    }
   ],
   "source": [
    "# get the feature importance scores\n",
    "weight_score = xgb_listwise.get_score(importance_type='weight')\n",
    "gain_score = xgb_listwise.get_score(importance_type='gain')\n",
    "cover_score = xgb_listwise.get_score(importance_type='cover')\n",
    "f_score = xgb_listwise.get_fscore()\n",
    "\n",
    "import operator \n",
    "thresh = 0.10 # minimum % contribution of F-score\n",
    "max_fscore = np.max(list(f_score.values()))\n",
    "significant_features = [k for (k, v) in sorted(f_score.items(), key=operator.itemgetter(1)) if v > thresh*max_fscore]\n",
    "\n",
    "signif_scores = [f_score[k] for k in significant_features]\n",
    "ypos = np.arange(len(signif_scores)) # y position\n",
    "plt.barh(ypos, signif_scores, align='center', alpha=0.5)\n",
    "plt.yticks(ypos, significant_features)\n",
    "plt.xlabel('F-score')\n",
    "plt.title('Feature F-scores')\n",
    "plt.show()\n",
    "\n",
    "print(\"{} features reduced to {}\".format(len(f_score), len(significant_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning depth\n",
      "3     train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0            0.726119          0.004512         0.816935        0.004212   \n",
      "1            0.730964          0.005276         0.819767        0.002515   \n",
      "2            0.736258          0.006040         0.823214        0.003989   \n",
      "3            0.739076          0.006640         0.825601        0.003998   \n",
      "4            0.741442          0.007557         0.828139        0.004134   \n",
      "5            0.741571          0.007182         0.827589        0.005626   \n",
      "6            0.744122          0.006834         0.828965        0.005400   \n",
      "7            0.745806          0.007500         0.830496        0.004985   \n",
      "8            0.747040          0.006242         0.831480        0.004390   \n",
      "9            0.747882          0.005408         0.831974        0.003636   \n",
      "10           0.749212          0.005634         0.832647        0.003977   \n",
      "11           0.750413          0.004377         0.833510        0.003300   \n",
      "12           0.751750          0.006034         0.834765        0.004056   \n",
      "13           0.752691          0.005701         0.835768        0.003709   \n",
      "14           0.754824          0.005991         0.837558        0.002699   \n",
      "15           0.754982          0.005658         0.837627        0.002190   \n",
      "16           0.754546          0.005532         0.837623        0.002329   \n",
      "17           0.756272          0.006693         0.838704        0.003219   \n",
      "\n",
      "    test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0           0.708991         0.019887        0.802972       0.014739  \n",
      "1           0.708886         0.014760        0.804226       0.011858  \n",
      "2           0.718159         0.012649        0.809053       0.010438  \n",
      "3           0.719359         0.018651        0.809868       0.011897  \n",
      "4           0.721604         0.018463        0.811854       0.012175  \n",
      "5           0.722176         0.016368        0.812597       0.010724  \n",
      "6           0.724148         0.013141        0.812567       0.010527  \n",
      "7           0.725448         0.012374        0.814734       0.008818  \n",
      "8           0.725369         0.014369        0.814643       0.009441  \n",
      "9           0.727269         0.011274        0.816165       0.007817  \n",
      "10          0.725497         0.012849        0.814957       0.008919  \n",
      "11          0.725731         0.014519        0.815368       0.009116  \n",
      "12          0.726919         0.015403        0.817033       0.008488  \n",
      "13          0.728807         0.016608        0.816853       0.008953  \n",
      "14          0.726989         0.015963        0.816409       0.009425  \n",
      "15          0.728465         0.015982        0.816825       0.009492  \n",
      "16          0.726731         0.017148        0.814963       0.009954  \n",
      "17          0.730344         0.017830        0.818488       0.010515  \n",
      "4    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.738408          0.007945         0.825424        0.005402   \n",
      "1           0.754102          0.012096         0.836481        0.007691   \n",
      "2           0.756875          0.011055         0.837639        0.006203   \n",
      "3           0.758499          0.007936         0.837969        0.005530   \n",
      "4           0.760754          0.007300         0.839690        0.005697   \n",
      "5           0.764745          0.009291         0.842265        0.007024   \n",
      "6           0.764256          0.010283         0.842639        0.007586   \n",
      "7           0.765692          0.009785         0.843238        0.006983   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.700382         0.017062        0.796619       0.015057  \n",
      "1          0.713366         0.016848        0.806400       0.011540  \n",
      "2          0.720955         0.019327        0.810119       0.012280  \n",
      "3          0.721102         0.023186        0.811251       0.015283  \n",
      "4          0.722054         0.018180        0.813268       0.010632  \n",
      "5          0.726534         0.018540        0.814549       0.010629  \n",
      "6          0.726105         0.018892        0.815080       0.010374  \n",
      "7          0.727736         0.020265        0.815662       0.012512  \n",
      "5    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.745340          0.004750         0.829996        0.004335   \n",
      "1           0.767856          0.007553         0.846455        0.005727   \n",
      "2           0.778008          0.009938         0.853212        0.006577   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.693639         0.018812        0.792749       0.011787  \n",
      "1          0.705158         0.015243        0.806075       0.012213  \n",
      "2          0.725207         0.018990        0.818172       0.009364  \n",
      "6    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.749903          0.006718         0.835637        0.003713   \n",
      "1           0.775152          0.005636         0.853201        0.004415   \n",
      "2           0.787190          0.005338         0.862317        0.005972   \n",
      "3           0.787950          0.006275         0.862441        0.003567   \n",
      "4           0.792249          0.006390         0.864213        0.003236   \n",
      "5           0.795245          0.005379         0.866653        0.002671   \n",
      "6           0.795422          0.003378         0.866340        0.001777   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.689742         0.021814        0.792363       0.011198  \n",
      "1          0.708527         0.010923        0.805170       0.005887  \n",
      "2          0.719755         0.008702        0.814310       0.008332  \n",
      "3          0.719983         0.006577        0.817151       0.006143  \n",
      "4          0.723635         0.007177        0.817788       0.006553  \n",
      "5          0.725882         0.007706        0.817089       0.008871  \n",
      "6          0.726968         0.011312        0.819149       0.011881  \n",
      "7    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.757452          0.007982         0.839097        0.004647   \n",
      "1           0.786541          0.008151         0.861364        0.005259   \n",
      "2           0.797072          0.006418         0.868586        0.004106   \n",
      "3           0.798668          0.009544         0.870470        0.005863   \n",
      "4           0.802507          0.008895         0.873333        0.005999   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.694788         0.016326        0.793729       0.012275  \n",
      "1          0.706962         0.016115        0.805913       0.007088  \n",
      "2          0.722289         0.011066        0.810447       0.005157  \n",
      "3          0.724810         0.010457        0.814749       0.001442  \n",
      "4          0.726318         0.016093        0.816101       0.008234  \n",
      "8    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.755254          0.010802         0.838605        0.005953   \n",
      "1           0.786669          0.012044         0.860907        0.006152   \n",
      "2           0.802212          0.010793         0.873297        0.006144   \n",
      "3           0.811155          0.011421         0.879083        0.006897   \n",
      "4           0.812587          0.009599         0.881533        0.006362   \n",
      "5           0.816257          0.008421         0.884994        0.005388   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.690057         0.015227        0.789750       0.011026  \n",
      "1          0.704047         0.006070        0.800753       0.005492  \n",
      "2          0.706409         0.016785        0.801072       0.009142  \n",
      "3          0.714031         0.006664        0.808477       0.006180  \n",
      "4          0.721408         0.009600        0.812303       0.007363  \n",
      "5          0.724665         0.013411        0.813674       0.008156  \n",
      "Best depth  6\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "for depth in range(3, 9):\n",
    "    params =  params = {'objective': 'rank:ndcg',\n",
    "              'eval_metric': ['ndcg@5','ndcg'],\n",
    "              'min_child_weight': 5,\n",
    "              'learning_rate': 0.1,\n",
    "              'max_depth': depth,\n",
    "              'gamma': 0.25,\n",
    "              'subsample': 1.0,\n",
    "              'colsample_bytree': 0.8\n",
    "              }\n",
    "\n",
    "    # early-stopping will evaluate again both test and train\n",
    "    search_results[depth] = xgb.cv(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, nfold=5)\n",
    "\n",
    "print(\"Tuning depth\")\n",
    "scores = {}\n",
    "for t, res in sorted(search_results.items()):\n",
    "    print(t, res)\n",
    "    # results will be one row per boosting iteration\n",
    "    scores[t] = res['test-ndcg-mean'].max()\n",
    "    \n",
    "best_depth = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"Best depth \", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning min child weight\n",
      "1     train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0            0.768937          0.010211         0.848736        0.008009   \n",
      "1            0.798789          0.010560         0.868716        0.007644   \n",
      "2            0.808514          0.011086         0.876100        0.007436   \n",
      "3            0.807619          0.013207         0.876220        0.009008   \n",
      "4            0.810883          0.012264         0.877540        0.008669   \n",
      "5            0.813952          0.010718         0.879924        0.007409   \n",
      "6            0.815118          0.008719         0.880294        0.006196   \n",
      "7            0.815387          0.008179         0.881155        0.005468   \n",
      "8            0.818008          0.008060         0.882274        0.005591   \n",
      "9            0.820944          0.007291         0.884429        0.005216   \n",
      "10           0.821645          0.008453         0.885482        0.006249   \n",
      "11           0.823319          0.008176         0.886429        0.006625   \n",
      "12           0.824394          0.008573         0.887658        0.006690   \n",
      "13           0.826080          0.007559         0.888694        0.006541   \n",
      "\n",
      "    test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0           0.696185         0.021392        0.795661       0.014487  \n",
      "1           0.712075         0.017715        0.803439       0.010967  \n",
      "2           0.719772         0.009281        0.808732       0.005140  \n",
      "3           0.727987         0.010957        0.815526       0.005673  \n",
      "4           0.731359         0.009124        0.817788       0.004271  \n",
      "5           0.732978         0.010661        0.817857       0.002339  \n",
      "6           0.731668         0.009284        0.817412       0.002036  \n",
      "7           0.730774         0.010704        0.818119       0.002424  \n",
      "8           0.734145         0.011821        0.819115       0.003376  \n",
      "9           0.732824         0.010503        0.818301       0.004853  \n",
      "10          0.731891         0.010144        0.817940       0.003843  \n",
      "11          0.732375         0.010130        0.818127       0.003493  \n",
      "12          0.733289         0.010250        0.818630       0.002703  \n",
      "13          0.734027         0.009521        0.819745       0.002004  \n",
      "3    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.760134          0.007724         0.842726        0.005120   \n",
      "1           0.784515          0.007064         0.860564        0.005410   \n",
      "2           0.797532          0.008699         0.868840        0.004729   \n",
      "3           0.799365          0.005662         0.870150        0.004581   \n",
      "4           0.803044          0.005759         0.872330        0.004510   \n",
      "5           0.804215          0.008919         0.873836        0.005944   \n",
      "6           0.806037          0.007924         0.874643        0.005819   \n",
      "7           0.807405          0.006268         0.875492        0.004342   \n",
      "8           0.809469          0.006998         0.876312        0.004785   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.696646         0.023821        0.795628       0.013114  \n",
      "1          0.704331         0.008415        0.799726       0.005447  \n",
      "2          0.717190         0.014682        0.808736       0.011849  \n",
      "3          0.724588         0.015703        0.814076       0.010485  \n",
      "4          0.724681         0.012827        0.813139       0.009414  \n",
      "5          0.728350         0.011112        0.815113       0.008179  \n",
      "6          0.726622         0.017195        0.814380       0.009322  \n",
      "7          0.730610         0.013443        0.817090       0.007357  \n",
      "8          0.729947         0.015792        0.817895       0.009152  \n",
      "5    train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0           0.752167          0.006402         0.836390        0.003984   \n",
      "1           0.775120          0.003396         0.853391        0.003936   \n",
      "2           0.782156          0.004254         0.857863        0.003394   \n",
      "3           0.785270          0.006966         0.859497        0.004542   \n",
      "4           0.786544          0.007162         0.861277        0.004795   \n",
      "5           0.790922          0.006325         0.864594        0.004218   \n",
      "6           0.793248          0.006280         0.865691        0.002984   \n",
      "\n",
      "   test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0          0.690756         0.023917        0.793692       0.011492  \n",
      "1          0.708682         0.009904        0.805322       0.003492  \n",
      "2          0.723293         0.007420        0.815427       0.004657  \n",
      "3          0.723673         0.007039        0.816753       0.005159  \n",
      "4          0.725218         0.014304        0.817423       0.008451  \n",
      "5          0.726938         0.009711        0.818586       0.006244  \n",
      "6          0.733227         0.010725        0.820138       0.007049  \n",
      "6     train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0            0.752322          0.008142         0.836112        0.006525   \n",
      "1            0.775534          0.005924         0.852181        0.004277   \n",
      "2            0.782729          0.008349         0.856982        0.007026   \n",
      "3            0.782133          0.006986         0.857178        0.004913   \n",
      "4            0.789163          0.004100         0.861783        0.003002   \n",
      "5            0.793368          0.005835         0.864544        0.004569   \n",
      "6            0.795627          0.006915         0.866743        0.005237   \n",
      "7            0.796258          0.006708         0.867584        0.004589   \n",
      "8            0.796857          0.005371         0.868627        0.003786   \n",
      "9            0.797670          0.004607         0.868950        0.003228   \n",
      "10           0.798401          0.005594         0.869882        0.003675   \n",
      "11           0.798497          0.004138         0.869724        0.002939   \n",
      "12           0.799101          0.005338         0.869951        0.003077   \n",
      "13           0.800645          0.004224         0.871284        0.002389   \n",
      "14           0.802764          0.003912         0.872799        0.002701   \n",
      "15           0.803469          0.003815         0.872774        0.002952   \n",
      "16           0.804205          0.004816         0.873306        0.003683   \n",
      "\n",
      "    test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0           0.702946         0.021505        0.801618       0.013840  \n",
      "1           0.707299         0.016949        0.802157       0.012422  \n",
      "2           0.719077         0.010848        0.810979       0.008064  \n",
      "3           0.715011         0.011106        0.806062       0.009557  \n",
      "4           0.718317         0.008717        0.807776       0.008032  \n",
      "5           0.719258         0.010602        0.810342       0.004451  \n",
      "6           0.724893         0.012769        0.813337       0.007904  \n",
      "7           0.724395         0.011578        0.811141       0.007599  \n",
      "8           0.723508         0.007985        0.812437       0.005923  \n",
      "9           0.722800         0.007319        0.810292       0.006149  \n",
      "10          0.723504         0.007762        0.811003       0.006318  \n",
      "11          0.722169         0.009190        0.810632       0.004430  \n",
      "12          0.719711         0.008183        0.810660       0.002640  \n",
      "13          0.719903         0.007660        0.810026       0.003336  \n",
      "14          0.720580         0.007446        0.809838       0.003083  \n",
      "15          0.722003         0.006177        0.811113       0.002206  \n",
      "16          0.724727         0.007633        0.814674       0.002067  \n",
      "7     train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0            0.746985          0.005914         0.832567        0.004671   \n",
      "1            0.773161          0.005432         0.849976        0.003959   \n",
      "2            0.784581          0.006601         0.858977        0.005163   \n",
      "3            0.788530          0.010448         0.861622        0.007109   \n",
      "4            0.790093          0.010824         0.862525        0.008090   \n",
      "5            0.793113          0.011275         0.864035        0.007285   \n",
      "6            0.795983          0.010617         0.865876        0.007651   \n",
      "7            0.796664          0.011415         0.866792        0.008823   \n",
      "8            0.795296          0.011596         0.866145        0.008727   \n",
      "9            0.797252          0.010381         0.867530        0.007638   \n",
      "10           0.798959          0.010952         0.868683        0.007886   \n",
      "11           0.800515          0.009067         0.869547        0.007088   \n",
      "12           0.802472          0.007878         0.871298        0.006180   \n",
      "13           0.803212          0.008413         0.872145        0.006370   \n",
      "\n",
      "    test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0           0.694966         0.015300        0.794829       0.012694  \n",
      "1           0.716062         0.023123        0.806230       0.012065  \n",
      "2           0.715080         0.025321        0.806794       0.012458  \n",
      "3           0.718786         0.022921        0.810497       0.012395  \n",
      "4           0.724953         0.021064        0.812801       0.011957  \n",
      "5           0.730864         0.017748        0.815229       0.010667  \n",
      "6           0.734008         0.019228        0.816336       0.011025  \n",
      "7           0.735055         0.019556        0.815289       0.010450  \n",
      "8           0.733586         0.017800        0.816168       0.011445  \n",
      "9           0.730501         0.016368        0.814080       0.011421  \n",
      "10          0.732149         0.013526        0.816939       0.010120  \n",
      "11          0.730421         0.011045        0.815641       0.008811  \n",
      "12          0.729675         0.010649        0.816620       0.005667  \n",
      "13          0.732001         0.016133        0.817756       0.010248  \n",
      "8     train-ndcg@5-mean  train-ndcg@5-std  train-ndcg-mean  train-ndcg-std  \\\n",
      "0            0.750057          0.007278         0.834127        0.004722   \n",
      "1            0.771774          0.002818         0.850156        0.002815   \n",
      "2            0.781919          0.006901         0.857588        0.006366   \n",
      "3            0.786484          0.007579         0.860580        0.006583   \n",
      "4            0.788747          0.010241         0.862257        0.008382   \n",
      "5            0.791004          0.010044         0.864384        0.008224   \n",
      "6            0.791837          0.010962         0.865897        0.008838   \n",
      "7            0.791492          0.012516         0.864423        0.009223   \n",
      "8            0.793712          0.013074         0.865470        0.009419   \n",
      "9            0.793707          0.013825         0.865675        0.009653   \n",
      "10           0.793403          0.013749         0.865466        0.009298   \n",
      "11           0.795154          0.012880         0.866650        0.008789   \n",
      "12           0.794506          0.011508         0.866666        0.008175   \n",
      "\n",
      "    test-ndcg@5-mean  test-ndcg@5-std  test-ndcg-mean  test-ndcg-std  \n",
      "0           0.691866         0.013530        0.794958       0.009669  \n",
      "1           0.707605         0.009238        0.802314       0.007842  \n",
      "2           0.717627         0.008741        0.807773       0.007407  \n",
      "3           0.724899         0.008388        0.812545       0.006096  \n",
      "4           0.726974         0.012643        0.816549       0.006350  \n",
      "5           0.726055         0.011087        0.813689       0.006996  \n",
      "6           0.725500         0.011814        0.812431       0.007696  \n",
      "7           0.724428         0.009423        0.812725       0.004652  \n",
      "8           0.724716         0.008358        0.812651       0.004479  \n",
      "9           0.724564         0.008543        0.814087       0.004716  \n",
      "10          0.724591         0.008142        0.815160       0.003503  \n",
      "11          0.727149         0.010701        0.815436       0.003429  \n",
      "12          0.728200         0.008808        0.816746       0.004537  \n",
      "Best weight 5\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "for min_child_weight in [1, 3, 5, 6, 7, 8]:\n",
    "    params = {'objective': 'rank:ndcg',\n",
    "              'eval_metric': ['ndcg@5', 'ndcg'],\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'learning_rate': 0.1,\n",
    "              'max_depth': best_depth,\n",
    "              'subsample': 1.0,\n",
    "              'colsample_bytree': 0.8\n",
    "              }\n",
    "\n",
    "    # early-stopping will evaluate again both test and train\n",
    "    search_results[min_child_weight] = xgb.cv(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, nfold=5)\n",
    "    \n",
    "print(\"Tuning min child weight\")\n",
    "scores = {}\n",
    "for t, res in sorted(search_results.items()):\n",
    "    print(t, res)\n",
    "    # results will be one row per boosting iteration\n",
    "    scores[t] = res['test-ndcg-mean'].max()\n",
    "    \n",
    "best_weight = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "print('Best weight', best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning gamma\n",
      "0    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836390        0.003984        0.793692       0.011492\n",
      "1         0.853391        0.003936        0.805322       0.003492\n",
      "2         0.857863        0.003394        0.815427       0.004657\n",
      "3         0.859497        0.004542        0.816753       0.005159\n",
      "4         0.861277        0.004795        0.817423       0.008451\n",
      "5         0.864594        0.004218        0.818586       0.006244\n",
      "6         0.865691        0.002984        0.820138       0.007049\n",
      "0.1     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.836050        0.004065        0.795246       0.012162\n",
      "1          0.852947        0.003059        0.806131       0.002965\n",
      "2          0.859013        0.004657        0.817164       0.002551\n",
      "3          0.859547        0.003645        0.816755       0.006027\n",
      "4          0.861497        0.004061        0.816618       0.003391\n",
      "5          0.864402        0.003688        0.819274       0.002960\n",
      "6          0.865575        0.003785        0.817501       0.001699\n",
      "7          0.865753        0.004380        0.818869       0.005400\n",
      "8          0.867395        0.002491        0.819697       0.008788\n",
      "9          0.868606        0.001793        0.819601       0.003861\n",
      "10         0.869298        0.001278        0.818753       0.003492\n",
      "11         0.870207        0.001105        0.818787       0.004690\n",
      "12         0.871628        0.001753        0.818855       0.005371\n",
      "13         0.871595        0.002663        0.818622       0.004335\n",
      "14         0.873179        0.002157        0.818380       0.002143\n",
      "15         0.873839        0.002861        0.818940       0.003008\n",
      "16         0.874481        0.002436        0.820617       0.003433\n",
      "17         0.875419        0.001730        0.820949       0.002707\n",
      "18         0.876472        0.002613        0.819503       0.002932\n",
      "19         0.877471        0.004122        0.821987       0.004792\n",
      "0.2    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.851785        0.004593        0.806431       0.003488\n",
      "2         0.860617        0.003398        0.818084       0.003999\n",
      "3         0.862533        0.002835        0.820019       0.003037\n",
      "4         0.864989        0.003128        0.819721       0.003003\n",
      "5         0.867511        0.001606        0.818726       0.005618\n",
      "6         0.867652        0.002302        0.820932       0.007726\n",
      "7         0.867942        0.001557        0.822170       0.008327\n",
      "0.3     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.835840        0.004000        0.792570       0.010957\n",
      "1          0.852593        0.003980        0.804693       0.005818\n",
      "2          0.862066        0.006028        0.815113       0.007999\n",
      "3          0.862426        0.003822        0.814929       0.006357\n",
      "4          0.864962        0.003551        0.813462       0.007922\n",
      "5          0.867725        0.003050        0.813709       0.007631\n",
      "6          0.868381        0.003053        0.817764       0.012346\n",
      "7          0.867994        0.002454        0.816008       0.008107\n",
      "8          0.869201        0.001735        0.817591       0.010047\n",
      "9          0.870486        0.001223        0.815562       0.009827\n",
      "10         0.870982        0.003052        0.818147       0.008232\n",
      "11         0.872165        0.002262        0.817877       0.009759\n",
      "12         0.872511        0.002594        0.818586       0.012020\n",
      "13         0.873873        0.003111        0.818709       0.010415\n",
      "14         0.874033        0.002230        0.818133       0.012754\n",
      "15         0.874460        0.002431        0.818089       0.014547\n",
      "16         0.874913        0.002193        0.819650       0.012828\n",
      "17         0.875764        0.001636        0.819678       0.014492\n",
      "18         0.876502        0.001934        0.819919       0.013649\n",
      "19         0.877152        0.001351        0.819077       0.014361\n",
      "20         0.878541        0.002351        0.818407       0.010971\n",
      "21         0.878580        0.002554        0.817731       0.010650\n",
      "22         0.879817        0.001826        0.817374       0.008863\n",
      "23         0.881465        0.001616        0.816334       0.008338\n",
      "24         0.882310        0.002437        0.818589       0.007434\n",
      "25         0.882981        0.002501        0.818498       0.012139\n",
      "26         0.884587        0.002728        0.820441       0.009902\n",
      "27         0.884797        0.002855        0.821598       0.009852\n",
      "0.4    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.834914        0.004635        0.793158       0.009606\n",
      "1         0.855412        0.001659        0.808232       0.009737\n",
      "2         0.859575        0.003368        0.818197       0.002408\n",
      "0.5    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.833784        0.004059        0.792367       0.011337\n",
      "1         0.855624        0.001649        0.804798       0.013578\n",
      "2         0.861640        0.004432        0.812597       0.012450\n",
      "3         0.864970        0.005131        0.812905       0.008092\n",
      "4         0.866518        0.006861        0.812676       0.011549\n",
      "5         0.868470        0.006645        0.812604       0.011436\n",
      "6         0.867605        0.007115        0.814984       0.010457\n",
      "Best gamma 0.2\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "for gamma in [0, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    params = {'objective': 'rank:ndcg',\n",
    "              'eval_metric': ['ndcg'],\n",
    "              'min_child_weight': best_weight,\n",
    "              'learning_rate': 0.1,\n",
    "              'max_depth': best_depth,\n",
    "              'gamma': gamma,\n",
    "              'subsample': 1.0,\n",
    "              'colsample_bytree': 0.8\n",
    "              }\n",
    "\n",
    "    # early-stopping will evaluate again both test and train\n",
    "    search_results[gamma] = xgb.cv(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, nfold=5)\n",
    "    \n",
    "print(\"Tuning gamma\")\n",
    "scores = {}\n",
    "for t, res in sorted(search_results.items()):\n",
    "    print(t, res)\n",
    "    # results will be one row per boosting iteration\n",
    "    scores[t] = res['test-ndcg-mean'].max()\n",
    "    \n",
    "best_gamma = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"Best gamma\", best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning subsamples\n",
      "(0.5, 0.5)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.807880        0.003318        0.783133       0.015999\n",
      "1          0.827477        0.002088        0.801914       0.013437\n",
      "2          0.833703        0.003146        0.808758       0.012872\n",
      "3          0.837413        0.002077        0.810914       0.011017\n",
      "4          0.840573        0.002000        0.809022       0.014526\n",
      "5          0.846508        0.003024        0.815036       0.014206\n",
      "6          0.849033        0.003164        0.813336       0.011262\n",
      "7          0.849342        0.003873        0.809598       0.011406\n",
      "8          0.851163        0.005033        0.814579       0.008519\n",
      "9          0.852685        0.005176        0.816285       0.008667\n",
      "10         0.855566        0.004326        0.816129       0.010785\n",
      "11         0.857216        0.003934        0.818696       0.009561\n",
      "12         0.857471        0.005104        0.819886       0.010577\n",
      "(0.5, 0.6)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.807868        0.003829        0.785528       0.016474\n",
      "1          0.826778        0.001703        0.795723       0.020560\n",
      "2          0.835325        0.003767        0.801688       0.014119\n",
      "3          0.839105        0.004492        0.805406       0.014838\n",
      "4          0.843746        0.005587        0.809682       0.017033\n",
      "5          0.847643        0.002394        0.810644       0.013864\n",
      "6          0.847963        0.001456        0.811445       0.011773\n",
      "7          0.850192        0.002718        0.813456       0.012447\n",
      "8          0.852240        0.002598        0.812057       0.014691\n",
      "9          0.852302        0.002839        0.815491       0.014199\n",
      "10         0.854057        0.003652        0.815778       0.012561\n",
      "11         0.855380        0.003611        0.814025       0.008660\n",
      "12         0.857466        0.003353        0.813724       0.009268\n",
      "13         0.857521        0.003329        0.815125       0.010422\n",
      "14         0.858570        0.002680        0.819993       0.008344\n",
      "(0.5, 0.7)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.808425        0.003371        0.787976       0.009624\n",
      "1          0.830293        0.004055        0.795183       0.005545\n",
      "2          0.838575        0.004096        0.801428       0.007840\n",
      "3          0.842270        0.004028        0.802365       0.005953\n",
      "4          0.844570        0.003876        0.807046       0.010642\n",
      "5          0.845990        0.002561        0.809411       0.012130\n",
      "6          0.849992        0.003760        0.809245       0.014303\n",
      "7          0.852942        0.002941        0.805679       0.011478\n",
      "8          0.853293        0.004322        0.804878       0.012946\n",
      "9          0.854284        0.003203        0.809656       0.010264\n",
      "10         0.855772        0.002293        0.809589       0.005851\n",
      "11         0.857334        0.001212        0.810038       0.008729\n",
      "12         0.858880        0.002145        0.814423       0.008977\n",
      "13         0.860081        0.000906        0.815357       0.006292\n",
      "14         0.860993        0.002031        0.815015       0.006012\n",
      "15         0.862343        0.002446        0.813121       0.006145\n",
      "16         0.862852        0.002101        0.811870       0.007014\n",
      "17         0.863460        0.001725        0.813819       0.008345\n",
      "18         0.864223        0.001805        0.817624       0.008447\n",
      "(0.5, 0.8)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.808482        0.002110        0.786469       0.007006\n",
      "1          0.834229        0.002026        0.792246       0.004049\n",
      "2          0.839140        0.003481        0.798864       0.008222\n",
      "3          0.843744        0.003336        0.803680       0.003414\n",
      "4          0.847797        0.003394        0.807753       0.004172\n",
      "5          0.849713        0.001435        0.809469       0.006602\n",
      "6          0.850194        0.003792        0.813617       0.009089\n",
      "7          0.853335        0.002927        0.812876       0.007464\n",
      "8          0.854747        0.002517        0.814363       0.009312\n",
      "9          0.855598        0.001347        0.815240       0.012784\n",
      "10         0.858517        0.001528        0.817596       0.009356\n",
      "11         0.857551        0.001159        0.816118       0.010859\n",
      "12         0.858045        0.003117        0.818294       0.010395\n",
      "13         0.860015        0.002304        0.817139       0.011389\n",
      "14         0.861361        0.002366        0.816545       0.013330\n",
      "15         0.862864        0.001677        0.818397       0.011778\n",
      "16         0.863256        0.002427        0.819300       0.011609\n",
      "17         0.863675        0.003434        0.817455       0.010799\n",
      "18         0.864844        0.002473        0.818167       0.009401\n",
      "19         0.865366        0.002916        0.816290       0.007191\n",
      "20         0.865926        0.002919        0.818321       0.006968\n",
      "21         0.867455        0.001584        0.819666       0.006800\n",
      "(0.5, 0.9)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.805600        0.005057        0.785436       0.012399\n",
      "1          0.830195        0.004946        0.800171       0.008126\n",
      "2          0.837475        0.005439        0.801728       0.008889\n",
      "3          0.842038        0.004548        0.808655       0.011197\n",
      "4          0.844217        0.005212        0.808449       0.013062\n",
      "5          0.846703        0.003013        0.808149       0.013134\n",
      "6          0.850803        0.002328        0.808323       0.014572\n",
      "7          0.853478        0.003324        0.810295       0.015192\n",
      "8          0.854961        0.002112        0.811931       0.014905\n",
      "9          0.856043        0.003413        0.813467       0.012873\n",
      "10         0.856981        0.001982        0.812832       0.011655\n",
      "11         0.857679        0.001570        0.811663       0.010227\n",
      "12         0.857133        0.001055        0.814124       0.011951\n",
      "13         0.857523        0.000891        0.813849       0.012811\n",
      "14         0.859093        0.001481        0.816056       0.011624\n",
      "15         0.859992        0.001109        0.814750       0.011145\n",
      "16         0.860892        0.001042        0.814374       0.011859\n",
      "17         0.862183        0.002015        0.818333       0.012047\n",
      "18         0.863839        0.001233        0.816561       0.012563\n",
      "19         0.865369        0.002397        0.814425       0.011314\n",
      "20         0.864826        0.001741        0.815403       0.012533\n",
      "21         0.866183        0.002518        0.816855       0.010524\n",
      "22         0.866433        0.001548        0.816202       0.007731\n",
      "23         0.867090        0.002157        0.817581       0.009268\n",
      "24         0.868028        0.002690        0.817980       0.010296\n",
      "25         0.867998        0.002135        0.818599       0.009871\n",
      "(0.5, 1.0)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.803565        0.005494        0.783349       0.008334\n",
      "1          0.826530        0.004734        0.793017       0.007508\n",
      "2          0.832724        0.007006        0.800803       0.004317\n",
      "3          0.841539        0.003034        0.804032       0.005333\n",
      "4          0.843322        0.004292        0.805706       0.005683\n",
      "5          0.845249        0.003547        0.809189       0.007473\n",
      "6          0.850118        0.004926        0.812326       0.005670\n",
      "7          0.852658        0.002822        0.809550       0.008314\n",
      "8          0.854317        0.003176        0.811339       0.007822\n",
      "9          0.855308        0.003392        0.812672       0.007120\n",
      "10         0.855834        0.004494        0.811628       0.008658\n",
      "11         0.856655        0.004995        0.811182       0.009942\n",
      "12         0.858840        0.005616        0.813676       0.007245\n",
      "13         0.858783        0.005139        0.813479       0.008050\n",
      "14         0.862230        0.004750        0.812721       0.009354\n",
      "15         0.863207        0.004882        0.814348       0.007591\n",
      "16         0.864982        0.006333        0.815389       0.008217\n",
      "17         0.867107        0.005904        0.814024       0.008646\n",
      "18         0.866739        0.006026        0.817051       0.011373\n",
      "19         0.866994        0.004712        0.816194       0.010959\n",
      "20         0.867321        0.005036        0.817651       0.010157\n",
      "(0.6, 0.5)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.816026        0.003631        0.783639       0.012121\n",
      "1          0.835560        0.003025        0.803154       0.008212\n",
      "2          0.842307        0.002143        0.800638       0.013048\n",
      "3          0.843431        0.004553        0.803250       0.012016\n",
      "4          0.848389        0.004916        0.805560       0.009388\n",
      "5          0.851660        0.004737        0.805480       0.011665\n",
      "6          0.853767        0.003414        0.805400       0.012552\n",
      "7          0.856287        0.004526        0.805169       0.011350\n",
      "8          0.858091        0.004705        0.809883       0.009871\n",
      "9          0.856138        0.003902        0.811839       0.005451\n",
      "10         0.859743        0.002996        0.811803       0.007500\n",
      "11         0.860388        0.003494        0.812061       0.006681\n",
      "12         0.859859        0.004741        0.812753       0.008148\n",
      "13         0.861464        0.003710        0.812239       0.006559\n",
      "14         0.864295        0.003289        0.812057       0.008341\n",
      "15         0.863986        0.003617        0.811362       0.006807\n",
      "16         0.866063        0.004316        0.812884       0.006484\n",
      "17         0.865966        0.004413        0.813096       0.007183\n",
      "18         0.865814        0.005141        0.813011       0.007950\n",
      "19         0.865601        0.005092        0.813717       0.006432\n",
      "20         0.866101        0.005060        0.814713       0.006964\n",
      "21         0.865996        0.005169        0.814368       0.007419\n",
      "22         0.867050        0.004666        0.814506       0.007900\n",
      "23         0.868664        0.005200        0.817212       0.008253\n",
      "24         0.867774        0.005138        0.817990       0.006884\n",
      "(0.6, 0.6)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.817014        0.003749        0.785315       0.012420\n",
      "1          0.832082        0.003540        0.801594       0.011506\n",
      "2          0.840679        0.005852        0.804241       0.010734\n",
      "3          0.843427        0.004020        0.802770       0.012056\n",
      "4          0.846298        0.006968        0.801096       0.013085\n",
      "5          0.849239        0.006510        0.802943       0.010235\n",
      "6          0.852536        0.006045        0.806036       0.009880\n",
      "7          0.855309        0.006652        0.809362       0.013070\n",
      "8          0.854708        0.005597        0.807193       0.012669\n",
      "9          0.855399        0.005461        0.806872       0.010839\n",
      "10         0.857808        0.004446        0.812931       0.011445\n",
      "11         0.858590        0.005027        0.811801       0.013126\n",
      "12         0.859737        0.004699        0.812763       0.010499\n",
      "13         0.860107        0.003292        0.812210       0.010209\n",
      "14         0.861199        0.003241        0.813202       0.010878\n",
      "15         0.862670        0.004510        0.813383       0.011672\n",
      "16         0.864222        0.004067        0.813597       0.011355\n",
      "17         0.865953        0.004363        0.812938       0.011194\n",
      "18         0.865196        0.003715        0.810996       0.009912\n",
      "19         0.866228        0.004195        0.813558       0.008663\n",
      "20         0.868277        0.004635        0.812629       0.008281\n",
      "21         0.868644        0.004671        0.815134       0.008528\n",
      "22         0.868495        0.003399        0.814709       0.006738\n",
      "23         0.869658        0.002869        0.814418       0.006941\n",
      "24         0.870362        0.002331        0.814397       0.008006\n",
      "25         0.870819        0.002377        0.814826       0.006348\n",
      "26         0.871338        0.002307        0.815050       0.007163\n",
      "27         0.872417        0.002318        0.815143       0.005915\n",
      "28         0.872050        0.002236        0.814510       0.007928\n",
      "29         0.872809        0.002336        0.815307       0.008015\n",
      "30         0.873171        0.001638        0.815867       0.008401\n",
      "31         0.873785        0.002326        0.815444       0.006585\n",
      "32         0.872785        0.002537        0.815700       0.006402\n",
      "33         0.874400        0.003189        0.815945       0.006222\n",
      "34         0.875148        0.002164        0.816283       0.005594\n",
      "35         0.875578        0.001682        0.816776       0.004642\n",
      "36         0.875894        0.001595        0.817251       0.004929\n",
      "37         0.876633        0.002154        0.816961       0.006415\n",
      "38         0.877788        0.002157        0.814228       0.007709\n",
      "39         0.878672        0.002240        0.816042       0.007698\n",
      "40         0.879191        0.002725        0.816361       0.006444\n",
      "41         0.879107        0.001677        0.817170       0.007523\n",
      "42         0.879331        0.002686        0.815849       0.008186\n",
      "43         0.880161        0.001707        0.816090       0.008006\n",
      "44         0.880977        0.002318        0.817878       0.006719\n",
      "45         0.881825        0.001437        0.817818       0.006941\n",
      "46         0.882027        0.001354        0.818027       0.007241\n",
      "(0.6, 0.7)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.811731        0.003907        0.773067       0.023230\n",
      "1          0.833384        0.003449        0.799890       0.013164\n",
      "2          0.840393        0.004078        0.803870       0.007575\n",
      "3          0.843815        0.002993        0.805084       0.008140\n",
      "4          0.846010        0.003988        0.806830       0.007311\n",
      "5          0.847880        0.003880        0.807726       0.006533\n",
      "6          0.850471        0.005908        0.811938       0.006600\n",
      "7          0.852403        0.005796        0.807340       0.008223\n",
      "8          0.854791        0.004250        0.807639       0.010209\n",
      "9          0.856010        0.003691        0.806047       0.009466\n",
      "10         0.856313        0.003324        0.811954       0.010314\n",
      "11         0.857281        0.003401        0.810931       0.009692\n",
      "12         0.857913        0.004191        0.810474       0.009948\n",
      "13         0.859788        0.004743        0.811404       0.007097\n",
      "14         0.861894        0.004615        0.812139       0.007878\n",
      "15         0.862250        0.003927        0.814382       0.009177\n",
      "16         0.861722        0.004557        0.813229       0.008891\n",
      "17         0.863088        0.003969        0.812957       0.008870\n",
      "18         0.864662        0.004715        0.813191       0.010429\n",
      "19         0.865943        0.003957        0.813560       0.009076\n",
      "20         0.867044        0.003986        0.814643       0.008959\n",
      "21         0.867785        0.003908        0.814547       0.008962\n",
      "22         0.868787        0.003364        0.814054       0.007895\n",
      "23         0.868764        0.003303        0.814258       0.006891\n",
      "24         0.869988        0.002832        0.814180       0.007836\n",
      "25         0.871030        0.002275        0.816617       0.011695\n",
      "26         0.871200        0.002168        0.818445       0.009736\n",
      "27         0.872980        0.002816        0.818191       0.008241\n",
      "28         0.872846        0.002850        0.817652       0.006761\n",
      "29         0.874156        0.002675        0.820262       0.010342\n",
      "30         0.874695        0.003647        0.818875       0.008938\n",
      "31         0.875180        0.003839        0.816792       0.006045\n",
      "32         0.875442        0.003787        0.816761       0.009521\n",
      "33         0.876049        0.003463        0.818625       0.010214\n",
      "34         0.875970        0.003605        0.819033       0.008911\n",
      "35         0.877327        0.003418        0.818713       0.008253\n",
      "36         0.878390        0.002635        0.820530       0.009778\n",
      "37         0.878890        0.002553        0.820607       0.008946\n",
      "(0.6, 0.8)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.819698        0.006120        0.794311       0.010101\n",
      "1          0.836668        0.004914        0.800538       0.010807\n",
      "2          0.845897        0.005911        0.806351       0.014676\n",
      "3          0.850384        0.004229        0.801693       0.014663\n",
      "4          0.850032        0.003956        0.809749       0.008836\n",
      "5          0.852822        0.004155        0.808237       0.007467\n",
      "6          0.855129        0.004894        0.808124       0.008938\n",
      "7          0.857992        0.006713        0.806652       0.012034\n",
      "8          0.859868        0.005938        0.809421       0.011682\n",
      "9          0.859980        0.005253        0.810415       0.011093\n",
      "10         0.861934        0.005543        0.809938       0.011414\n",
      "11         0.862959        0.006499        0.809320       0.012942\n",
      "12         0.862695        0.006488        0.811233       0.011123\n",
      "13         0.864446        0.005942        0.811645       0.012618\n",
      "14         0.866592        0.005959        0.814201       0.009662\n",
      "15         0.866629        0.005278        0.815778       0.008689\n",
      "16         0.867729        0.004702        0.815154       0.009330\n",
      "17         0.867566        0.004914        0.814740       0.010383\n",
      "18         0.868910        0.004651        0.818204       0.010036\n",
      "(0.6, 0.9)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.813969        0.006580        0.783425       0.013130\n",
      "1          0.838820        0.003653        0.803643       0.006044\n",
      "2          0.844400        0.005144        0.801436       0.005414\n",
      "3          0.847176        0.005958        0.803327       0.007665\n",
      "4          0.848756        0.005596        0.803030       0.007605\n",
      "5          0.852508        0.006371        0.806807       0.005520\n",
      "6          0.853864        0.005924        0.807952       0.005890\n",
      "7          0.856378        0.007892        0.803039       0.009137\n",
      "8          0.857722        0.006732        0.804477       0.009221\n",
      "9          0.859900        0.004858        0.804884       0.010893\n",
      "10         0.860501        0.004936        0.804964       0.010941\n",
      "11         0.861022        0.005758        0.806571       0.012163\n",
      "12         0.862624        0.004715        0.810047       0.011439\n",
      "(0.6, 1.0)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.814292        0.004268        0.787605       0.012056\n",
      "1          0.834032        0.003428        0.799527       0.009985\n",
      "2          0.842259        0.003289        0.806271       0.007120\n",
      "3          0.846206        0.005152        0.807854       0.004081\n",
      "4          0.848401        0.005242        0.805132       0.005421\n",
      "5          0.851967        0.006613        0.807403       0.003963\n",
      "6          0.855253        0.006901        0.806070       0.005024\n",
      "7          0.857514        0.006323        0.803669       0.005405\n",
      "8          0.858585        0.004862        0.804056       0.007719\n",
      "9          0.858586        0.002679        0.805873       0.008068\n",
      "10         0.859885        0.003414        0.808795       0.006746\n",
      "11         0.862254        0.003492        0.808476       0.006553\n",
      "12         0.862277        0.003829        0.808491       0.007121\n",
      "13         0.862598        0.003930        0.811288       0.008883\n",
      "14         0.864166        0.004329        0.812698       0.007975\n",
      "15         0.864760        0.004407        0.813777       0.006792\n",
      "16         0.864673        0.004807        0.812930       0.005872\n",
      "17         0.866433        0.006078        0.811725       0.005328\n",
      "18         0.867496        0.004691        0.811594       0.006891\n",
      "19         0.869108        0.004790        0.812246       0.005942\n",
      "20         0.870108        0.004525        0.811238       0.005445\n",
      "21         0.870169        0.003594        0.810910       0.004304\n",
      "22         0.871534        0.004263        0.813385       0.006311\n",
      "23         0.872608        0.004565        0.813809       0.005299\n",
      "24         0.871967        0.003898        0.815514       0.005959\n",
      "25         0.873902        0.003522        0.816997       0.006293\n",
      "26         0.874694        0.003858        0.817585       0.007663\n",
      "27         0.875847        0.003856        0.816382       0.005705\n",
      "28         0.876322        0.004266        0.816754       0.004708\n",
      "29         0.876571        0.005121        0.818649       0.004189\n",
      "(0.7, 0.5)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.821902        0.004975        0.782563       0.012772\n",
      "1          0.839586        0.004507        0.795940       0.011406\n",
      "2          0.848077        0.002607        0.797958       0.007954\n",
      "3          0.849003        0.003611        0.803471       0.008261\n",
      "4          0.851758        0.003651        0.808461       0.007533\n",
      "5          0.855591        0.001923        0.808214       0.008147\n",
      "6          0.857869        0.002197        0.813026       0.010719\n",
      "7          0.859611        0.004895        0.813228       0.009736\n",
      "8          0.861598        0.004916        0.813596       0.009629\n",
      "9          0.861570        0.004847        0.817037       0.010065\n",
      "10         0.861383        0.005297        0.815086       0.009997\n",
      "11         0.862898        0.005034        0.813912       0.008013\n",
      "12         0.864407        0.004633        0.815016       0.008769\n",
      "13         0.865298        0.005074        0.814401       0.007754\n",
      "14         0.866879        0.004936        0.817089       0.008210\n",
      "15         0.868767        0.005901        0.817302       0.007879\n",
      "(0.7, 0.6)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.820473        0.006438        0.774501       0.024120\n",
      "1          0.837731        0.006169        0.798021       0.023678\n",
      "2          0.847524        0.003591        0.805342       0.017251\n",
      "3          0.852361        0.003875        0.811276       0.007924\n",
      "4          0.856484        0.006250        0.807034       0.008164\n",
      "5          0.858651        0.004401        0.808444       0.006559\n",
      "6          0.859441        0.005081        0.809898       0.006556\n",
      "7          0.862760        0.006605        0.805681       0.007591\n",
      "8          0.863279        0.005793        0.809408       0.004989\n",
      "9          0.864390        0.006239        0.811578       0.005410\n",
      "10         0.865608        0.006922        0.811938       0.004898\n",
      "11         0.864892        0.006259        0.810720       0.002939\n",
      "12         0.865186        0.005945        0.812742       0.004436\n",
      "13         0.866020        0.005401        0.811040       0.004241\n",
      "14         0.867267        0.005712        0.811716       0.003727\n",
      "15         0.868736        0.005026        0.811653       0.006376\n",
      "16         0.869615        0.004115        0.813070       0.006130\n",
      "17         0.869568        0.004599        0.813021       0.004885\n",
      "18         0.869634        0.004927        0.815218       0.003672\n",
      "19         0.870652        0.004231        0.814305       0.004467\n",
      "20         0.871889        0.004953        0.814900       0.006229\n",
      "21         0.871542        0.005315        0.814851       0.006275\n",
      "22         0.873016        0.004935        0.815710       0.006894\n",
      "(0.7, 0.7)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.823878        0.010758        0.787111       0.020208\n",
      "1          0.842050        0.009028        0.795655       0.013211\n",
      "2          0.849299        0.003583        0.800922       0.013224\n",
      "3          0.854431        0.005059        0.804163       0.008713\n",
      "4          0.856573        0.003496        0.808874       0.009666\n",
      "5          0.859319        0.002575        0.805796       0.009204\n",
      "6          0.861380        0.003874        0.809411       0.012996\n",
      "7          0.864455        0.004709        0.811124       0.012678\n",
      "8          0.864232        0.005944        0.811200       0.012573\n",
      "9          0.864555        0.007032        0.808677       0.010593\n",
      "10         0.866450        0.006332        0.810327       0.012413\n",
      "11         0.868007        0.006922        0.807772       0.011906\n",
      "12         0.868990        0.006643        0.810190       0.011578\n",
      "13         0.869255        0.004691        0.812011       0.008768\n",
      "14         0.870535        0.005633        0.812142       0.008806\n",
      "15         0.869908        0.006088        0.813237       0.009288\n",
      "16         0.870847        0.006532        0.815338       0.010353\n",
      "17         0.871482        0.006135        0.813178       0.007471\n",
      "18         0.871752        0.005554        0.811617       0.008022\n",
      "19         0.873073        0.006552        0.812768       0.008263\n",
      "20         0.874701        0.006446        0.813272       0.008971\n",
      "21         0.875376        0.006990        0.815286       0.010131\n",
      "22         0.875102        0.006219        0.814036       0.006918\n",
      "23         0.877044        0.007520        0.814331       0.008542\n",
      "24         0.877522        0.007009        0.812702       0.008066\n",
      "25         0.878349        0.006145        0.814101       0.009300\n",
      "26         0.879640        0.005940        0.815374       0.008421\n",
      "27         0.880227        0.006319        0.814666       0.007269\n",
      "28         0.880762        0.006674        0.815927       0.008851\n",
      "29         0.881431        0.006265        0.816254       0.008362\n",
      "30         0.882572        0.005891        0.817104       0.008411\n",
      "(0.7, 0.8)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.821075        0.007514        0.788192       0.007359\n",
      "1         0.844113        0.005160        0.798103       0.010176\n",
      "2         0.848422        0.003487        0.810421       0.011323\n",
      "3         0.851281        0.002471        0.809428       0.015850\n",
      "4         0.854254        0.004528        0.808387       0.012692\n",
      "5         0.854850        0.005234        0.807207       0.009908\n",
      "6         0.858369        0.003676        0.809641       0.010801\n",
      "7         0.861850        0.005665        0.805773       0.011424\n",
      "8         0.862784        0.004082        0.809504       0.011198\n",
      "9         0.863324        0.003669        0.811909       0.010318\n",
      "(0.7, 0.9)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.820435        0.003731        0.786979       0.012183\n",
      "1          0.841330        0.002141        0.793704       0.009905\n",
      "2          0.848861        0.005246        0.803060       0.011022\n",
      "3          0.855629        0.003936        0.803165       0.013344\n",
      "4          0.857422        0.003122        0.808334       0.009207\n",
      "5          0.859805        0.002893        0.806640       0.011619\n",
      "6          0.861561        0.004184        0.806378       0.010073\n",
      "7          0.863289        0.004541        0.807690       0.010518\n",
      "8          0.866086        0.003980        0.810992       0.012675\n",
      "9          0.867488        0.004662        0.810232       0.010864\n",
      "10         0.868516        0.004835        0.808676       0.013001\n",
      "11         0.868662        0.005190        0.809871       0.009988\n",
      "12         0.869497        0.005125        0.811269       0.009648\n",
      "13         0.869399        0.004313        0.813177       0.011520\n",
      "14         0.870458        0.003938        0.814306       0.012649\n",
      "(0.7, 1.0)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.818472        0.003389        0.785428       0.012645\n",
      "1          0.841068        0.005757        0.795182       0.013474\n",
      "2          0.847793        0.003449        0.806134       0.010778\n",
      "3          0.851540        0.003954        0.808804       0.006706\n",
      "4          0.854407        0.003837        0.810093       0.007338\n",
      "5          0.856355        0.004632        0.809364       0.008106\n",
      "6          0.859218        0.005117        0.808784       0.006182\n",
      "7          0.860835        0.005716        0.809854       0.005819\n",
      "8          0.862213        0.004920        0.810337       0.007275\n",
      "9          0.863257        0.004445        0.810277       0.006035\n",
      "10         0.864176        0.004590        0.810070       0.006880\n",
      "11         0.866042        0.004640        0.809076       0.008458\n",
      "12         0.864975        0.004524        0.807428       0.009213\n",
      "13         0.867649        0.004865        0.806696       0.005478\n",
      "14         0.869164        0.004190        0.808498       0.006558\n",
      "15         0.869605        0.004947        0.809598       0.006915\n",
      "16         0.870505        0.004081        0.810011       0.006050\n",
      "17         0.872456        0.004989        0.809986       0.005770\n",
      "18         0.871694        0.005272        0.811787       0.006953\n",
      "19         0.873419        0.005378        0.811838       0.007479\n",
      "20         0.873630        0.004793        0.809623       0.006908\n",
      "21         0.874815        0.004842        0.809335       0.005237\n",
      "22         0.874912        0.004969        0.811890       0.005752\n",
      "23         0.875968        0.004823        0.812407       0.006915\n",
      "24         0.877005        0.005032        0.811879       0.007566\n",
      "25         0.877567        0.005002        0.811987       0.006832\n",
      "26         0.877259        0.004575        0.812751       0.008912\n",
      "27         0.877676        0.004558        0.811111       0.006682\n",
      "28         0.878059        0.004382        0.811781       0.007708\n",
      "29         0.878603        0.004325        0.812234       0.006947\n",
      "30         0.879518        0.004788        0.811842       0.006672\n",
      "31         0.879960        0.003641        0.814911       0.006501\n",
      "32         0.880584        0.003468        0.813932       0.005211\n",
      "33         0.881078        0.003910        0.815301       0.007398\n",
      "34         0.882064        0.003451        0.813161       0.007861\n",
      "35         0.881934        0.003791        0.813533       0.006162\n",
      "36         0.883066        0.003882        0.813914       0.006168\n",
      "37         0.884692        0.003463        0.813481       0.006893\n",
      "38         0.886029        0.004357        0.812442       0.006055\n",
      "39         0.886733        0.003703        0.813952       0.007376\n",
      "40         0.886411        0.003850        0.817205       0.006097\n",
      "41         0.887707        0.002899        0.816264       0.006185\n",
      "42         0.887876        0.002764        0.816047       0.006831\n",
      "43         0.889036        0.003196        0.816756       0.007374\n",
      "44         0.889981        0.003327        0.815313       0.007731\n",
      "45         0.890076        0.003138        0.817834       0.005173\n",
      "46         0.890327        0.003065        0.816343       0.004311\n",
      "47         0.890396        0.002925        0.816704       0.004607\n",
      "48         0.891691        0.003496        0.817776       0.004778\n",
      "49         0.891665        0.003412        0.818581       0.006531\n",
      "50         0.891865        0.003563        0.819097       0.006150\n",
      "51         0.892916        0.003848        0.818834       0.006442\n",
      "52         0.892839        0.003597        0.817859       0.005519\n",
      "53         0.893797        0.005255        0.816879       0.005929\n",
      "54         0.895185        0.005058        0.818346       0.006932\n",
      "55         0.896247        0.005411        0.818032       0.008930\n",
      "56         0.896605        0.005452        0.819610       0.009529\n",
      "(0.8, 0.5)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.821729        0.007275        0.786209       0.014524\n",
      "1          0.842532        0.004910        0.798830       0.011565\n",
      "2          0.846146        0.002029        0.804617       0.012665\n",
      "3          0.851285        0.005459        0.802482       0.012561\n",
      "4          0.854616        0.005767        0.805270       0.011138\n",
      "5          0.858082        0.006302        0.810298       0.010503\n",
      "6          0.861009        0.006188        0.811617       0.006638\n",
      "7          0.862257        0.005750        0.813145       0.007411\n",
      "8          0.863636        0.004691        0.812908       0.008316\n",
      "9          0.865898        0.005526        0.811891       0.010340\n",
      "10         0.867058        0.005268        0.812150       0.007698\n",
      "11         0.868029        0.004482        0.809517       0.009128\n",
      "12         0.868015        0.004668        0.812542       0.011138\n",
      "13         0.870199        0.005353        0.813587       0.011038\n",
      "(0.8, 0.6)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.822160        0.006586        0.784824       0.015844\n",
      "1          0.842509        0.005875        0.798874       0.008288\n",
      "2          0.853177        0.004741        0.802233       0.009948\n",
      "3          0.856878        0.005579        0.804734       0.003262\n",
      "4          0.857716        0.007166        0.808384       0.005006\n",
      "5          0.861815        0.008435        0.811686       0.007529\n",
      "6          0.864711        0.006348        0.810917       0.005692\n",
      "7          0.865700        0.005606        0.810682       0.007311\n",
      "8          0.869286        0.006750        0.809704       0.009399\n",
      "9          0.869398        0.005754        0.812764       0.009157\n",
      "10         0.870832        0.006749        0.813465       0.009538\n",
      "11         0.872263        0.005772        0.814239       0.007961\n",
      "12         0.873205        0.006168        0.813492       0.008405\n",
      "13         0.874300        0.007066        0.815820       0.009236\n",
      "14         0.874689        0.006187        0.818472       0.009694\n",
      "15         0.875504        0.006299        0.819534       0.010541\n",
      "16         0.876423        0.005881        0.817461       0.009317\n",
      "17         0.876360        0.005352        0.818882       0.009991\n",
      "18         0.877068        0.004672        0.818826       0.008829\n",
      "19         0.877789        0.004900        0.821057       0.010145\n",
      "(0.8, 0.7)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.822516        0.008730        0.783945       0.014237\n",
      "1         0.840486        0.008597        0.796957       0.015682\n",
      "2         0.852287        0.006988        0.806361       0.010946\n",
      "3         0.854796        0.007054        0.803739       0.011072\n",
      "4         0.857752        0.007346        0.807859       0.011803\n",
      "5         0.858445        0.008783        0.810921       0.010204\n",
      "(0.8, 0.8)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.826563        0.006580        0.783772       0.010706\n",
      "1          0.844706        0.008467        0.793756       0.008944\n",
      "2          0.852566        0.007761        0.799743       0.015369\n",
      "3          0.856688        0.007682        0.807067       0.010340\n",
      "4          0.858987        0.006342        0.807070       0.010205\n",
      "5          0.861735        0.006439        0.808039       0.008491\n",
      "6          0.866187        0.006060        0.813118       0.007857\n",
      "7          0.865303        0.007516        0.812392       0.009349\n",
      "8          0.868407        0.006886        0.811249       0.008831\n",
      "9          0.869680        0.005611        0.810895       0.007384\n",
      "10         0.869015        0.006423        0.811191       0.009675\n",
      "11         0.869791        0.005918        0.812104       0.008157\n",
      "12         0.870282        0.006509        0.811704       0.008226\n",
      "13         0.872044        0.005462        0.816273       0.006974\n",
      "(0.8, 0.9)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.823211        0.006684        0.781340       0.011304\n",
      "1          0.840679        0.006339        0.795368       0.009989\n",
      "2          0.850597        0.002971        0.800973       0.010053\n",
      "3          0.853716        0.004349        0.803442       0.008607\n",
      "4          0.857056        0.005353        0.804874       0.004991\n",
      "..              ...             ...             ...            ...\n",
      "59         0.900860        0.002973        0.818034       0.005453\n",
      "60         0.901858        0.001884        0.818457       0.006592\n",
      "61         0.902170        0.001862        0.819606       0.006059\n",
      "62         0.903315        0.002187        0.818864       0.005845\n",
      "63         0.903406        0.002541        0.820954       0.004462\n",
      "\n",
      "[64 rows x 4 columns]\n",
      "(0.8, 1.0)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.825156        0.006897        0.783847       0.012639\n",
      "1          0.842803        0.005909        0.795911       0.009376\n",
      "2          0.855434        0.004340        0.800223       0.008479\n",
      "3          0.858530        0.004315        0.802153       0.007823\n",
      "4          0.861553        0.004121        0.802117       0.006790\n",
      "5          0.863369        0.005934        0.805044       0.007171\n",
      "6          0.865548        0.005963        0.803427       0.011389\n",
      "7          0.866855        0.007187        0.804743       0.007310\n",
      "8          0.868741        0.008122        0.806051       0.005336\n",
      "9          0.869258        0.008600        0.807731       0.005571\n",
      "10         0.869458        0.007954        0.807520       0.005803\n",
      "11         0.869286        0.007651        0.808349       0.005312\n",
      "12         0.869840        0.007925        0.809014       0.006563\n",
      "13         0.870665        0.007691        0.807043       0.005761\n",
      "14         0.872555        0.008644        0.807457       0.007132\n",
      "15         0.872405        0.008814        0.809012       0.006443\n",
      "16         0.872911        0.008233        0.808456       0.006500\n",
      "17         0.872951        0.007812        0.809903       0.006670\n",
      "18         0.875021        0.008256        0.807340       0.008325\n",
      "19         0.875391        0.008307        0.810996       0.008676\n",
      "20         0.876245        0.007889        0.810562       0.008158\n",
      "21         0.876977        0.006854        0.811185       0.006885\n",
      "22         0.876923        0.006628        0.811662       0.007917\n",
      "23         0.877105        0.006331        0.810858       0.007558\n",
      "24         0.878939        0.005092        0.811481       0.006796\n",
      "25         0.881023        0.005483        0.811937       0.007522\n",
      "26         0.880911        0.005280        0.810750       0.006066\n",
      "27         0.881753        0.005887        0.810421       0.005924\n",
      "28         0.882314        0.004807        0.811856       0.006869\n",
      "29         0.883344        0.004459        0.812755       0.006978\n",
      "30         0.883857        0.004231        0.813531       0.006206\n",
      "31         0.884867        0.004180        0.810974       0.007008\n",
      "32         0.885043        0.003725        0.812460       0.007910\n",
      "33         0.886059        0.003747        0.812966       0.007595\n",
      "34         0.886598        0.003490        0.815118       0.007762\n",
      "(0.9, 0.5)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.824220        0.006029        0.787212       0.014642\n",
      "1         0.846868        0.005557        0.801406       0.010056\n",
      "2         0.852873        0.004353        0.808305       0.010997\n",
      "3         0.856024        0.003916        0.810072       0.007201\n",
      "4         0.858518        0.004369        0.808724       0.006593\n",
      "5         0.861277        0.005603        0.812426       0.003827\n",
      "(0.9, 0.6)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.826909        0.007393        0.784716       0.020622\n",
      "1          0.848388        0.005930        0.804470       0.006600\n",
      "2          0.858269        0.006396        0.805588       0.010460\n",
      "3          0.860551        0.006961        0.808412       0.007103\n",
      "4          0.861345        0.006830        0.806729       0.002920\n",
      "5          0.862551        0.006595        0.805831       0.005011\n",
      "6          0.863324        0.003876        0.809444       0.005576\n",
      "7          0.864961        0.004138        0.809000       0.006058\n",
      "8          0.864659        0.004743        0.811867       0.006309\n",
      "9          0.866027        0.004904        0.811993       0.004164\n",
      "10         0.866768        0.004310        0.812437       0.005437\n",
      "11         0.869416        0.004993        0.808984       0.004614\n",
      "12         0.869716        0.004626        0.811907       0.004728\n",
      "13         0.869541        0.004245        0.814592       0.005641\n",
      "(0.9, 0.7)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.829186        0.006853        0.789973       0.013844\n",
      "1          0.848422        0.006232        0.801773       0.013414\n",
      "2          0.859708        0.004467        0.807783       0.009158\n",
      "3          0.863121        0.005434        0.811977       0.006572\n",
      "4          0.865534        0.004983        0.810024       0.008756\n",
      "5          0.865294        0.003886        0.808815       0.003687\n",
      "6          0.867020        0.003555        0.810856       0.004445\n",
      "7          0.867463        0.004431        0.812925       0.006903\n",
      "8          0.869486        0.004714        0.812238       0.007222\n",
      "9          0.870695        0.004075        0.813326       0.006059\n",
      "10         0.872604        0.003956        0.815136       0.005975\n",
      "(0.9, 0.8)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.830217        0.005692        0.793344       0.013782\n",
      "1         0.850504        0.004791        0.810057       0.010462\n",
      "2         0.859785        0.002870        0.817298       0.013261\n",
      "3         0.863055        0.001550        0.815436       0.014688\n",
      "4         0.865990        0.002110        0.815287       0.011911\n",
      "5         0.868097        0.001838        0.817243       0.013909\n",
      "6         0.868633        0.002353        0.817188       0.013504\n",
      "7         0.869197        0.003022        0.817828       0.010538\n",
      "(0.9, 0.9)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.832338        0.005664        0.796392       0.009482\n",
      "1         0.853155        0.005409        0.802129       0.012275\n",
      "2         0.858186        0.004135        0.810935       0.011594\n",
      "3         0.860914        0.005254        0.812759       0.007773\n",
      "4         0.862928        0.004004        0.814274       0.007775\n",
      "(0.9, 1.0)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.830536        0.004313        0.794679       0.009238\n",
      "1         0.847565        0.003680        0.802143       0.007143\n",
      "2         0.855287        0.004180        0.807186       0.009430\n",
      "3         0.857935        0.005646        0.811521       0.007549\n",
      "4         0.858171        0.005845        0.808364       0.007164\n",
      "5         0.861854        0.005933        0.813293       0.008004\n",
      "(1.0, 0.5)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.830272        0.001972        0.781787       0.021342\n",
      "1          0.850367        0.004447        0.799975       0.014400\n",
      "2          0.860903        0.003184        0.802754       0.008167\n",
      "3          0.864671        0.002602        0.806473       0.007070\n",
      "4          0.869377        0.003514        0.805548       0.009721\n",
      "5          0.871757        0.003478        0.807764       0.011783\n",
      "6          0.872395        0.004503        0.806195       0.011341\n",
      "7          0.873080        0.005935        0.806998       0.007488\n",
      "8          0.872838        0.005253        0.808059       0.007515\n",
      "9          0.872681        0.004790        0.808528       0.008906\n",
      "10         0.873580        0.004224        0.809359       0.005639\n",
      "11         0.875780        0.004145        0.809642       0.008244\n",
      "12         0.876281        0.005065        0.809336       0.007467\n",
      "13         0.877322        0.005505        0.810655       0.006905\n",
      "14         0.878913        0.005213        0.809423       0.008273\n",
      "15         0.879599        0.005182        0.810167       0.006927\n",
      "16         0.879860        0.004712        0.810786       0.007429\n",
      "17         0.880408        0.004218        0.811074       0.006459\n",
      "18         0.880721        0.004174        0.811096       0.005146\n",
      "19         0.880958        0.004474        0.811724       0.005325\n",
      "20         0.882506        0.003648        0.810432       0.007419\n",
      "21         0.882647        0.003972        0.810380       0.007356\n",
      "22         0.884457        0.004286        0.811774       0.007340\n",
      "23         0.884093        0.003889        0.811622       0.007368\n",
      "24         0.884265        0.004226        0.811060       0.007834\n",
      "25         0.884589        0.003841        0.810850       0.007020\n",
      "26         0.885370        0.003801        0.813503       0.006729\n",
      "27         0.886112        0.003359        0.813905       0.006227\n",
      "28         0.887028        0.003303        0.814592       0.007203\n",
      "29         0.888368        0.003201        0.812812       0.006465\n",
      "30         0.888186        0.003450        0.814515       0.006896\n",
      "31         0.889237        0.003301        0.815545       0.006937\n",
      "32         0.889214        0.003248        0.816013       0.007443\n",
      "33         0.890738        0.003586        0.817431       0.006632\n",
      "34         0.890975        0.003765        0.817362       0.006255\n",
      "35         0.891901        0.004009        0.816584       0.006419\n",
      "36         0.892105        0.004411        0.816382       0.006410\n",
      "37         0.893146        0.003525        0.817326       0.006798\n",
      "38         0.893882        0.003416        0.817590       0.006415\n",
      "39         0.893732        0.004102        0.816545       0.007510\n",
      "40         0.894104        0.003615        0.817607       0.007135\n",
      "(1.0, 0.6)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.831802        0.004822        0.783869       0.020152\n",
      "1         0.847803        0.006709        0.800630       0.012255\n",
      "2         0.859088        0.004300        0.806571       0.008833\n",
      "3         0.864807        0.006015        0.810238       0.006460\n",
      "(1.0, 0.7)     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.834843        0.004354        0.789384       0.003762\n",
      "1          0.852592        0.007402        0.804362       0.005377\n",
      "2          0.862010        0.004755        0.807075       0.007385\n",
      "3          0.864715        0.006166        0.805103       0.006260\n",
      "4          0.867485        0.003507        0.806585       0.006412\n",
      "5          0.869847        0.004836        0.808018       0.004468\n",
      "6          0.870097        0.006763        0.810874       0.004605\n",
      "7          0.870884        0.006480        0.810173       0.004484\n",
      "8          0.871334        0.007198        0.811462       0.006927\n",
      "9          0.871869        0.007506        0.811602       0.006312\n",
      "10         0.872511        0.007122        0.812662       0.006545\n",
      "11         0.872762        0.006683        0.812191       0.005729\n",
      "12         0.873538        0.006628        0.812482       0.004103\n",
      "13         0.873597        0.007020        0.813007       0.003355\n",
      "14         0.874922        0.007539        0.812206       0.005406\n",
      "15         0.875384        0.006576        0.813736       0.005484\n",
      "16         0.876587        0.006058        0.812772       0.004648\n",
      "17         0.877238        0.006564        0.813584       0.004683\n",
      "18         0.877896        0.006447        0.812811       0.003466\n",
      "19         0.878695        0.006537        0.813149       0.004373\n",
      "20         0.879827        0.006566        0.813507       0.005095\n",
      "21         0.881332        0.005443        0.814341       0.005156\n",
      "(1.0, 0.8)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.851785        0.004593        0.806431       0.003488\n",
      "2         0.860617        0.003398        0.818084       0.003999\n",
      "3         0.862533        0.002835        0.820019       0.003037\n",
      "4         0.864989        0.003128        0.819721       0.003003\n",
      "5         0.867511        0.001606        0.818726       0.005618\n",
      "6         0.867652        0.002302        0.820932       0.007726\n",
      "7         0.867942        0.001557        0.822170       0.008327\n",
      "(1.0, 0.9)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.838216        0.003345        0.792949       0.012210\n",
      "1         0.852698        0.007736        0.806549       0.007745\n",
      "2         0.858536        0.008516        0.812853       0.008592\n",
      "3         0.861179        0.006249        0.811506       0.008674\n",
      "4         0.861050        0.004963        0.811369       0.010190\n",
      "5         0.863793        0.002760        0.811066       0.008018\n",
      "6         0.866593        0.004219        0.810224       0.005655\n",
      "7         0.867767        0.004536        0.813484       0.005993\n",
      "8         0.869201        0.005786        0.813844       0.004685\n",
      "(1.0, 1.0)    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.838381        0.005002        0.788892       0.013090\n",
      "1         0.852839        0.005212        0.802686       0.005141\n",
      "2         0.861580        0.003328        0.804040       0.002905\n",
      "3         0.864752        0.003511        0.807754       0.003973\n",
      "4         0.866844        0.004946        0.809871       0.005067\n",
      "5         0.866158        0.005768        0.808388       0.006980\n",
      "6         0.867270        0.003160        0.810594       0.005786\n",
      "Subsmaple by tree, column:  1.0 0.8\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "for subsample in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    for subsample_by_tree in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        params = {'objective': 'rank:ndcg',\n",
    "                  'eval_metric': ['ndcg'],\n",
    "                  'min_child_weight': best_weight,\n",
    "                  'learning_rate': 0.1,\n",
    "                  'max_depth': best_depth,\n",
    "                  'gamma': best_gamma,\n",
    "                  'subsample': subsample,\n",
    "                  'colsample_bytree': subsample_by_tree\n",
    "                  }\n",
    "\n",
    "        # early-stopping will evaluate again both test and train\n",
    "        search_results[(subsample, subsample_by_tree)] = xgb.cv(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, nfold=5)\n",
    "        \n",
    "print(\"Tuning subsamples\")\n",
    "scores = {}\n",
    "for t, res in sorted(search_results.items()):\n",
    "    print(t, res)\n",
    "    # results will be one row per boosting iteration\n",
    "    scores[t] = res['test-ndcg-mean'].max()\n",
    "    \n",
    "best_subsample, best_colsample = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"Subsmaple by tree, column: \", best_subsample, best_colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning learning rate\n",
      "0.05    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.850261        0.004467        0.806593       0.005075\n",
      "2         0.858570        0.004855        0.816854       0.007223\n",
      "3         0.859245        0.004319        0.813469       0.009271\n",
      "4         0.862058        0.003569        0.811879       0.005408\n",
      "5         0.863985        0.002597        0.817410       0.003551\n",
      "6         0.865782        0.002814        0.816620       0.002017\n",
      "7         0.866125        0.001943        0.817905       0.002765\n",
      "0.08    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.850821        0.004794        0.805386       0.003753\n",
      "2         0.858774        0.004310        0.816844       0.005325\n",
      "3         0.861201        0.002775        0.817646       0.006377\n",
      "4         0.863179        0.002433        0.818942       0.004759\n",
      "5         0.864930        0.002329        0.819617       0.005165\n",
      "6         0.864362        0.002431        0.819465       0.006143\n",
      "7         0.865513        0.002070        0.819518       0.005352\n",
      "8         0.867008        0.002683        0.820369       0.003705\n",
      "0.1    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.851785        0.004593        0.806431       0.003488\n",
      "2         0.860617        0.003398        0.818084       0.003999\n",
      "3         0.862533        0.002835        0.820019       0.003037\n",
      "4         0.864989        0.003128        0.819721       0.003003\n",
      "5         0.867511        0.001606        0.818726       0.005618\n",
      "6         0.867652        0.002302        0.820932       0.007726\n",
      "7         0.867942        0.001557        0.822170       0.008327\n",
      "0.15    train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0         0.836363        0.003511        0.793330       0.010192\n",
      "1         0.851750        0.004464        0.804781       0.005380\n",
      "2         0.860470        0.002342        0.813845       0.005568\n",
      "3         0.861307        0.003510        0.817444       0.005053\n",
      "4         0.865876        0.003286        0.818501       0.006640\n",
      "0.2     train-ndcg-mean  train-ndcg-std  test-ndcg-mean  test-ndcg-std\n",
      "0          0.836363        0.003511        0.793330       0.010192\n",
      "1          0.852484        0.005382        0.805179       0.010042\n",
      "2          0.859281        0.003742        0.807916       0.010546\n",
      "3          0.862325        0.001801        0.811110       0.007931\n",
      "4          0.864736        0.004351        0.813801       0.006964\n",
      "5          0.867906        0.004363        0.810478       0.006798\n",
      "6          0.871102        0.004832        0.810472       0.005399\n",
      "7          0.871748        0.004353        0.814821       0.008098\n",
      "8          0.874322        0.003662        0.816202       0.007444\n",
      "9          0.876257        0.003657        0.817492       0.010072\n",
      "10         0.878241        0.004481        0.820437       0.013066\n",
      "Best learning rate 0.1\n"
     ]
    }
   ],
   "source": [
    "search_results = {}\n",
    "for learning_rate in [0.05, 0.08, 0.10, 0.15, 0.20]:\n",
    "    params = {'objective': 'rank:ndcg',\n",
    "              'eval_metric': ['ndcg'],\n",
    "              'min_child_weight': best_weight,\n",
    "              'learning_rate': learning_rate,\n",
    "              'max_depth': best_depth,\n",
    "              'gamma': best_gamma,\n",
    "              'subsample': best_subsample,\n",
    "              'colsample_bytree': best_colsample\n",
    "              }\n",
    "\n",
    "    # early-stopping will evaluate again both test and train\n",
    "    search_results[learning_rate] = xgb.cv(params, Dtrain, num_boost_round=2500, early_stopping_rounds=10, nfold=5)\n",
    "    \n",
    "print(\"Tuning learning rate\")\n",
    "scores = {}\n",
    "for t, res in sorted(search_results.items()):\n",
    "    print(t, res)\n",
    "    # results will be one row per boosting iteration\n",
    "    scores[t] = res['test-ndcg-mean'].max()\n",
    "    \n",
    "best_learning_rate = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"Best learning rate\", best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum parameters:  {'min_child_weight': 5, 'learning_rate': 0.1, 'max_depth': 6, 'gamma': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "params = {'min_child_weight': best_weight,\n",
    "          'learning_rate': best_learning_rate,\n",
    "          'max_depth': best_depth,\n",
    "          'gamma': best_gamma,\n",
    "          'subsample': best_subsample,\n",
    "          'colsample_bytree': best_colsample\n",
    "          }\n",
    "\n",
    "print(\"Optimum parameters: \", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
